{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is heavily inspired by [link](https://www.youtube.com/watch?v=aywZrzNaKjs&ab_channel=Rabbitmetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are neural networks trained on large datasets of natural language text, used to generate predictions about the probability of a given word or phrase appearing in a sentence.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a python script that does sagemaker batch processing inside lambda function\")\n",
    "]\n",
    "response=chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample Python script that performs SageMaker batch processing inside a Lambda function:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import os\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    # Set up the SageMaker client\n",
      "    sm_client = boto3.client('sagemaker')\n",
      "\n",
      "    # Set up the input and output S3 locations\n",
      "    input_s3_uri = 's3://<input-bucket>/<input-prefix>'\n",
      "    output_s3_uri = 's3://<output-bucket>/<output-prefix>'\n",
      "\n",
      "    # Set up the batch job parameters\n",
      "    job_name = '<batch-job-name>'\n",
      "    job_input = {\n",
      "        'S3InputMode': 'File',\n",
      "        'S3Uri': input_s3_uri,\n",
      "        'InputMode': 'File',\n",
      "        'ContentType': 'text/csv'\n",
      "    }\n",
      "    job_output = {\n",
      "        'S3OutputPath': output_s3_uri,\n",
      "        'OutputName': 'output'\n",
      "    }\n",
      "    job_args = {\n",
      "        'ModelName': '<model-name>',\n",
      "        'MaxPayloadInMB': 6\n",
      "    }\n",
      "    job_config = {\n",
      "        'BatchStrategy': 'MultiRecord',\n",
      "        'MaxConcurrentTransforms': 0,\n",
      "        'MaxPayloadInMB': 6\n",
      "    }\n",
      "\n",
      "    # Start the batch job\n",
      "    response = sm_client.create_transform_job(\n",
      "        TransformJobName=job_name,\n",
      "        ModelName=job_args['ModelName'],\n",
      "        MaxPayloadInMB=job_args['MaxPayloadInMB'],\n",
      "        BatchStrategy=job_config['BatchStrategy'],\n",
      "        MaxConcurrentTransforms=job_config['MaxConcurrentTransforms'],\n",
      "        TransformInput=job_input,\n",
      "        TransformOutput=job_output\n",
      "    )\n",
      "\n",
      "    # Return the batch job response\n",
      "    return response\n",
      "```\n",
      "\n",
      "Note that you will need to replace the placeholders in the script with your own values, such as the input and output S3 bucket names and prefixes, the batch job name, the model name, and any other relevant parameters. Also, make sure that your Lambda function has the necessary permissions to access the SageMaker service and the input and output S3 buckets.\n"
     ]
    }
   ],
   "source": [
    "print(response.content, end='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROMT TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expetise in building ML model in AWS ecosystem.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "promt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expetise in building ML model in AWS ecosystem.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBatch Transform is an AWS SageMaker feature that enables the user to quickly and cost-effectively process large amounts of data in a single request. It allows for batch inference on an entire dataset using an existing ML model. This means that it can quickly generate inferences on a large dataset without the need to manually pre-process or set up a separate training job.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(promt.format(concept=\"batch transform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel registry is a centralized platform for managing and tracking the different versions of ML models. It provides a single source of truth for recording and tracking model versions, data sets, and model-related artifacts. This helps organizations to manage model lineage, track model performance, and enable reproducibility.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HERE we dynamically change the promt with user input\n",
    "llm(promt.format(concept=\"model registry\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAIN\n",
    "It takes LLM and Template and combines them into an interface that takes input from the user and returnds output from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoders are a type of neural network that is used for unsupervised learning. It is an unsupervised algorithm that takes an input, encodes it into a hidden representation, and then decodes the hidden representation back to its original form. Autoencoders are used for a variety of tasks such as dimensionality reduction, anomaly detection, and image reconstruction.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=promt)\n",
    "\n",
    "# Run the chain only specidying the input vatiable\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second chain takes the output of the first chain as input and does an action on it (in this case it \n",
    "# takes a concept from first chain and explains it to me like I'm fine years old)\n",
    "\n",
    "second_promt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Trun the concept description of {ml_concept} and explain it to me like I am five in 100 words\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "An autoencoder is a type of artificial neural network used to learn efficient representations of input data, called \"codings\", by reconstructing the input data from the codings. It is designed to reduce the dimensionality of the input data while preserving the important features of the data in the codings.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "An autoencoder is a type of machine learning. It helps computers to figure out patterns in data. It works by taking in a set of data and compressing it into a smaller set of numbers, called codings. This helps computers to understand the data better, without losing any important information. The codings can then be used to reconstruct the data, which is how the autoencoder learns. It's a great way of reducing a lot of data into something much more manageable.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "An autoencoder is a type of machine learning. It helps computers to figure out patterns in data. It works by taking in a set of data and compressing it into a smaller set of numbers, called codings. This helps computers to understand the data better, without losing any important information. The codings can then be used to reconstruct the data, which is how the autoencoder learns. It's a great way of reducing a lot of data into something much more manageable.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and VectorStores\n",
    "Split the explanation into chunks to be stored in VectorStore - \"pincone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "text = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='An autoencoder is a type of machine learning. It', metadata={}),\n",
       " Document(page_content='helps computers to figure out patterns in data. It', metadata={}),\n",
       " Document(page_content='works by taking in a set of data and compressing', metadata={}),\n",
       " Document(page_content='it into a smaller set of numbers, called codings.', metadata={}),\n",
       " Document(page_content='This helps computers to understand the data', metadata={}),\n",
       " Document(page_content='better, without losing any important information.', metadata={}),\n",
       " Document(page_content='The codings can then be used to reconstruct the', metadata={}),\n",
       " Document(page_content=\"data, which is how the autoencoder learns. It's a\", metadata={}),\n",
       " Document(page_content='great way of reducing a lot of data into something', metadata={}),\n",
       " Document(page_content='much more manageable.', metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autoencoder is a type of machine learning. It'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03356080542448993, 0.02426007791594276, 0.015185698117317289, 0.016173385277295175, 0.0388078954991406]\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# vector representation of 0s embedding\n",
    "query_results = embeddings.embed_query(text[0].page_content)\n",
    "print(query_results[:5])\n",
    "print(len(query_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENV')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pinecone' from '/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create an index first; use the lenght of 0s embedding as a dimension\n",
    "dimension = len(query_results)\n",
    "index_name = \"langchain-quickstart\"\n",
    "pinecone.create_index(index_name, dimension=dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Pinecone.from_documents(text, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='An autoencoder is a type of machine learning. It', metadata={}),\n",
       " Document(page_content=\"data, which is how the autoencoder learns. It's a\", metadata={}),\n",
       " Document(page_content='This helps computers to understand the data', metadata={}),\n",
       " Document(page_content='The codings can then be used to reconstruct the', metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the relevant chunks are extracted\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=500),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to solve a quadratic equation\n",
      "Action: Python REPL\n",
      "Action Input: import numpy as np\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the numpy function to solve the equation\n",
      "Action: Python REPL\n",
      "Action Input: np.roots([3, 2, -1])\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: (-1.0, 0.3333333333333333)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(-1.0, 0.3333333333333333)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows LLM to run python code\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x - 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the index since free version of Pinecode allows only one index to be stored\n",
    "index_name = \"langchain-quickstart\"\n",
    "pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
