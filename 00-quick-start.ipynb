{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is heavily inspired by [link](https://www.youtube.com/watch?v=aywZrzNaKjs&ab_channel=Rabbitmetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are neural networks trained on large datasets of natural language text, used to generate predictions about the probability of a given word or phrase appearing in a sentence.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a python script that does sagemaker batch processing inside lambda function\")\n",
    "]\n",
    "response=chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example Python script that does SageMaker batch processing inside a Lambda function:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "\n",
      "def lambda_handler(event, context):\n",
      "    # Set up the SageMaker client\n",
      "    sagemaker = boto3.client('sagemaker')\n",
      "\n",
      "    # Set up the batch processing job parameters\n",
      "    job_name = 'my-batch-processing-job'\n",
      "    input_location = 's3://my-batch-processing-input-bucket/input/'\n",
      "    output_location = 's3://my-batch-processing-output-bucket/output/'\n",
      "    job_definition = 'my-batch-processing-job-definition'\n",
      "    job_queue = 'my-batch-processing-job-queue'\n",
      "\n",
      "    # Start the batch processing job\n",
      "    response = sagemaker.create_processing_job(\n",
      "        ProcessingInputs=[\n",
      "            {\n",
      "                'InputName': 'input',\n",
      "                'S3Input': {\n",
      "                    'S3Uri': input_location,\n",
      "                    'LocalPath': '/opt/ml/processing/input',\n",
      "                    'S3DataType': 'S3Prefix',\n",
      "                    'S3InputMode': 'File',\n",
      "                    'S3DataDistributionType': 'FullyReplicated'\n",
      "                }\n",
      "            },\n",
      "        ],\n",
      "        ProcessingOutputConfig={\n",
      "            'Outputs': [\n",
      "                {\n",
      "                    'OutputName': 'output',\n",
      "                    'S3Output': {\n",
      "                        'S3Uri': output_location,\n",
      "                        'LocalPath': '/opt/ml/processing/output',\n",
      "                        'S3UploadMode': 'EndOfJob'\n",
      "                    }\n",
      "                },\n",
      "            ]\n",
      "        },\n",
      "        ProcessingJobName=job_name,\n",
      "        ProcessingResources={\n",
      "            'ClusterConfig': {\n",
      "                'InstanceCount': 1,\n",
      "                'InstanceType': 'ml.m5.large',\n",
      "                'VolumeSizeInGB': 30\n",
      "            }\n",
      "        },\n",
      "        StoppingCondition={\n",
      "            'MaxRuntimeInSeconds': 3600\n",
      "        },\n",
      "        AppSpecification={\n",
      "            'ImageUri': '123456789012.dkr.ecr.us-west-2.amazonaws.com/my-batch-processing-image:latest',\n",
      "            'ContainerEntrypoint': [\n",
      "                'python3',\n",
      "                '/opt/ml/processing/code/batch_processing_script.py'\n",
      "            ]\n",
      "        },\n",
      "        Environment={\n",
      "            'MY_ENV_VAR': 'my-env-var-value'\n",
      "        },\n",
      "        NetworkConfig={\n",
      "            'EnableNetworkIsolation': False\n",
      "        },\n",
      "        RoleArn='arn:aws:iam::123456789012:role/my-batch-processing-role',\n",
      "        Tags=[\n",
      "            {\n",
      "                'Key': 'my-tag-key',\n",
      "                'Value': 'my-tag-value'\n",
      "            },\n",
      "        ],\n",
      "        ExperimentConfig={\n",
      "            'ExperimentName': 'my-experiment',\n",
      "            'TrialName': 'my-trial',\n",
      "            'TrialComponentDisplayName': 'my-trial-component'\n",
      "        }\n",
      "    )\n",
      "\n",
      "    # Return the response\n",
      "    return response\n",
      "```\n",
      "\n",
      "This script creates a SageMaker processing job with the specified input and output locations, job definition, job queue, and other parameters. The processing job runs a container with the specified image URI and container entrypoint, passing in any specified environment variables. The processing job is run with the specified IAM role and tags, and can be associated with an experiment and trial for tracking purposes. The response from the `create_processing_job` call is returned from the Lambda function.\n"
     ]
    }
   ],
   "source": [
    "print(response.content, end='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROMT TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expetise in building ML model in AWS ecosystem.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "promt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expetise in building ML model in AWS ecosystem.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBatch Transform is a feature of AWS that allows you to run an ML model on a batch of data. It enables you to quickly and efficiently get predictions from an ML model on large amounts of data without having to manually set up the environment. You simply upload your data, point to the ML model, and the batch transform job will run in the background.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(promt.format(concept=\"batch transform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel registry is a concept that allows for the tracking and managing of ML models throughout their entire lifecycles. It provides a centralized repository for tracking models, their data sources, their performance metrics, and the data and code used to create them. Additionally, it enables versioning and governance of ML models, making it easier to understand, replicate, and reuse models.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HERE we dynamically change the promt with user input\n",
    "llm(promt.format(concept=\"model registry\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHAIN\n",
    "It takes LLM and Template and combines them into an interface that takes input from the user and returnds output from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An autoencoder is a type of artificial neural network used to learn efficient data representations in an unsupervised manner. It consists of an encoder which compresses data into a low-dimensional representation, and a decoder which reconstructs the input data from the compressed representation. Autoencoders are commonly used for dimensionality reduction, denoising, and feature extraction.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=promt)\n",
    "\n",
    "# Run the chain only specidying the input vatiable\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second chain takes the output of the first chain as input and does an action on it (in this case it \n",
    "# takes a concept from first chain and explains it to me like I'm fine years old)\n",
    "\n",
    "second_promt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Trun the concept description of {ml_concept} and explain it to me like I am five in 100 words\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Autoencoders are unsupervised learning algorithms that attempt to reconstruct their input. They are used for dimensionality reduction, feature extraction, and representation learning. Autoencoders consist of two parts: an encoder and a decoder. The encoder compresses the input into a latent space representation, while the decoder reconstructs the input from the latent space representation.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Autoencoders are like machines that take something in and do something to make it smaller. They do this by making a copy of the thing they take in and then changing it so that it takes up less space. The machine then remembers how it changed the thing and it uses that information to put it back together again. It's like taking a jigsaw puzzle apart and then putting it back together again, but without ever seeing the picture on the box. Autoencoders are useful for making things smaller so it takes up less space, like shrinking a photo or making a long sentence shorter.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Autoencoders are like machines that take something in and do something to make it smaller. They do this by making a copy of the thing they take in and then changing it so that it takes up less space. The machine then remembers how it changed the thing and it uses that information to put it back together again. It's like taking a jigsaw puzzle apart and then putting it back together again, but without ever seeing the picture on the box. Autoencoders are useful for making things smaller so it takes up less space, like shrinking a photo or making a long sentence shorter.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and VectorStores\n",
    "Split the explanation into chunks to be stored in VectorStore - \"pincone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "text = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Autoencoders are like machines that take something', metadata={}),\n",
       " Document(page_content='in and do something to make it smaller. They do', metadata={}),\n",
       " Document(page_content='this by making a copy of the thing they take in', metadata={}),\n",
       " Document(page_content='and then changing it so that it takes up less', metadata={}),\n",
       " Document(page_content='space. The machine then remembers how it changed', metadata={}),\n",
       " Document(page_content='the thing and it uses that information to put it', metadata={}),\n",
       " Document(page_content=\"back together again. It's like taking a jigsaw\", metadata={}),\n",
       " Document(page_content='puzzle apart and then putting it back together', metadata={}),\n",
       " Document(page_content='again, but without ever seeing the picture on the', metadata={}),\n",
       " Document(page_content='box. Autoencoders are useful for making things', metadata={}),\n",
       " Document(page_content='smaller so it takes up less space, like shrinking', metadata={}),\n",
       " Document(page_content='a photo or making a long sentence shorter.', metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Autoencoders are like machines that take something'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02128082520253445, 0.029654279441033268, 0.02945004785038224, 0.014173601796930103, 0.013928525192000475]\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# vector representation of 0s embedding\n",
    "query_results = embeddings.embed_query(text[0].page_content)\n",
    "print(query_results[:5])\n",
    "print(len(query_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENV')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pinecone' from '/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=UTF-8', 'date': 'Sat, 20 May 2023 21:10:35 GMT', 'x-envoy-upstream-service-time': '706', 'content-length': '131', 'server': 'envoy'})\nHTTP response body: The index exceeds the project quota of 1 pods by 1 pods. Upgrade your account or change the project settings to increase the quota.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dimension \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(query_results)\n\u001b[1;32m      3\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlangchain-quickstart\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pinecone\u001b[39m.\u001b[39;49mcreate_index(index_name, dimension\u001b[39m=\u001b[39;49mdimension)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/manage.py:118\u001b[0m, in \u001b[0;36mcreate_index\u001b[0;34m(name, dimension, timeout, index_type, metric, replicas, shards, pods, pod_type, index_config, metadata_config, source_collection)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a Pinecone index.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[39m:param name: the name of the index.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m:param timeout: Timeout for wait until index gets ready. If None, wait indefinitely; if >=0, time out after this many seconds; if -1, return immediately and do not wait. Default: None\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m api_instance \u001b[39m=\u001b[39m _get_api_instance()\n\u001b[0;32m--> 118\u001b[0m api_instance\u001b[39m.\u001b[39;49mcreate_index(create_request\u001b[39m=\u001b[39;49mCreateRequest(\n\u001b[1;32m    119\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    120\u001b[0m     dimension\u001b[39m=\u001b[39;49mdimension,\n\u001b[1;32m    121\u001b[0m     index_type\u001b[39m=\u001b[39;49mindex_type,\n\u001b[1;32m    122\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    123\u001b[0m     replicas\u001b[39m=\u001b[39;49mreplicas,\n\u001b[1;32m    124\u001b[0m     shards\u001b[39m=\u001b[39;49mshards,\n\u001b[1;32m    125\u001b[0m     pods\u001b[39m=\u001b[39;49mpods,\n\u001b[1;32m    126\u001b[0m     pod_type\u001b[39m=\u001b[39;49mpod_type,\n\u001b[1;32m    127\u001b[0m     index_config\u001b[39m=\u001b[39;49mindex_config \u001b[39mor\u001b[39;49;00m {},\n\u001b[1;32m    128\u001b[0m     metadata_config\u001b[39m=\u001b[39;49mmetadata_config,\n\u001b[1;32m    129\u001b[0m     source_collection\u001b[39m=\u001b[39;49msource_collection\n\u001b[1;32m    130\u001b[0m ))\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_ready\u001b[39m():\n\u001b[1;32m    133\u001b[0m     status \u001b[39m=\u001b[39m _get_status(name)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:776\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    766\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m \n\u001b[1;32m    775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallable(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api/index_operations_api.py:370\u001b[0m, in \u001b[0;36mIndexOperationsApi.__init__.<locals>.__create_index\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_check_return_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\n\u001b[1;32m    367\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m_check_return_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_with_http_info(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:838\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     header_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mselect_header_content_type(\n\u001b[1;32m    835\u001b[0m         content_type_headers_list)\n\u001b[1;32m    836\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m header_list\n\u001b[0;32m--> 838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[1;32m    839\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mendpoint_path\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mhttp_method\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    840\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    841\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    842\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    843\u001b[0m     body\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    844\u001b[0m     post_params\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mform\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    845\u001b[0m     files\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    846\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mresponse_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    847\u001b[0m     auth_settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mauth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    848\u001b[0m     async_req\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    849\u001b[0m     _check_type\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_check_return_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    850\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    851\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    852\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    853\u001b[0m     _host\u001b[39m=\u001b[39;49m_host,\n\u001b[1;32m    854\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mcollection_format\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:413\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[1;32m    414\u001b[0m                            path_params, query_params, header_params,\n\u001b[1;32m    415\u001b[0m                            body, post_params, files,\n\u001b[1;32m    416\u001b[0m                            response_type, auth_settings,\n\u001b[1;32m    417\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[1;32m    418\u001b[0m                            _preload_content, _request_timeout, _host,\n\u001b[1;32m    419\u001b[0m                            _check_type)\n\u001b[1;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    422\u001b[0m                                                method, path_params,\n\u001b[1;32m    423\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                                _request_timeout,\n\u001b[1;32m    432\u001b[0m                                                _host, _check_type))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:207\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[1;32m    211\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:200\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    196\u001b[0m     url \u001b[39m=\u001b[39m _host \u001b[39m+\u001b[39m resource_path\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    201\u001b[0m         method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[1;32m    202\u001b[0m         post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    203\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    204\u001b[0m         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/api_client.py:459\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mOPTIONS(url,\n\u001b[1;32m    452\u001b[0m                                     query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    453\u001b[0m                                     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                     _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    457\u001b[0m                                     body\u001b[39m=\u001b[39mbody)\n\u001b[1;32m    458\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mPOST(url,\n\u001b[1;32m    460\u001b[0m                                  query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    461\u001b[0m                                  headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    462\u001b[0m                                  post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[1;32m    463\u001b[0m                                  _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    464\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    465\u001b[0m                                  body\u001b[39m=\u001b[39;49mbody)\n\u001b[1;32m    466\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPUT\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mPUT(url,\n\u001b[1;32m    468\u001b[0m                                 query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m    469\u001b[0m                                 headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[1;32m    473\u001b[0m                                 body\u001b[39m=\u001b[39mbody)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/rest.py:271\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPOST\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, post_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    270\u001b[0m          body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[1;32m    272\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    273\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[1;32m    274\u001b[0m                         post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[1;32m    275\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[1;32m    276\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[1;32m    277\u001b[0m                         body\u001b[39m=\u001b[39;49mbody)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langchain-sandbox/lib/python3.11/site-packages/pinecone/core/client/rest.py:230\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m500\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m599\u001b[39m:\n\u001b[1;32m    228\u001b[0m         \u001b[39mraise\u001b[39;00m ServiceException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mraise\u001b[39;00m ApiException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=UTF-8', 'date': 'Sat, 20 May 2023 21:10:35 GMT', 'x-envoy-upstream-service-time': '706', 'content-length': '131', 'server': 'envoy'})\nHTTP response body: The index exceeds the project quota of 1 pods by 1 pods. Upgrade your account or change the project settings to increase the quota.\n"
     ]
    }
   ],
   "source": [
    "# we need to create an index first; use the lenght of 0s embedding as a dimension\n",
    "dimension = len(query_results)\n",
    "index_name = \"langchain-quickstart\"\n",
    "pinecone.create_index(index_name, dimension=dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Pinecone.from_documents(text, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='An autoencoder is a type of machine learning. It', metadata={}),\n",
       " Document(page_content=\"data, which is how the autoencoder learns. It's a\", metadata={}),\n",
       " Document(page_content='box. Autoencoders are useful for making things', metadata={}),\n",
       " Document(page_content='Autoencoders are like machines that take something', metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the relevant chunks are extracted\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=500),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to solve a quadratic equation\n",
      "Action: Python REPL\n",
      "Action Input: import numpy as np\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the numpy function to solve the equation\n",
      "Action: Python REPL\n",
      "Action Input: np.roots([3, 2, -1])\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: (-1.0, 0.3333333333333333)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(-1.0, 0.3333333333333333)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows LLM to run python code\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x - 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the index since free version of Pinecode allows only one index to be stored\n",
    "index_name = \"langchain-quickstart\"\n",
    "pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
